{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Py3 research env",
      "language": "python",
      "name": "py3_research"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Copy of practice01_simple_text_processing.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lagom-QB/M12/blob/master/practice01_simple_text_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sxIff7iTyPH",
        "colab_type": "text"
      },
      "source": [
        "## Practice 01. Simple text processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ijdw6mz9TyPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "from IPython import display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8eR3l80TyPk",
        "colab_type": "text"
      },
      "source": [
        "### Toxic or not\n",
        "Our main goal in this assignment is to classify, whether the comments are toxic or not. And practice with both classical approaches and PyTorch in the process.\n",
        "\n",
        "*Credits: This homework is inspired by YSDA NLP_course.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5kjQylwTyPr",
        "colab_type": "code",
        "outputId": "428988b9-6fc2-40d1-e8c0-17321e0040fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "# In colab uncomment this cell\n",
        "! wget https://raw.githubusercontent.com/neychev/harbour_dlia2020/master/day01/utils.py -nc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘utils.py’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9duPfoaTyP6",
        "colab_type": "code",
        "outputId": "a7c1843d-f70d-47e7-86f4-0cf15c74a8e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "try:\n",
        "    data = pd.read_csv('../datasets/comments_small_dataset/comments.tsv', sep='\\t')\n",
        "except FileNotFoundError:\n",
        "    ! wget https://raw.githubusercontent.com/neychev/harbour_dlia2020/master/datasets/comments_small_dataset/comments.tsv -nc\n",
        "    data = pd.read_csv(\"comments.tsv\", sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘comments.tsv’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_6l5FGaTyQL",
        "colab_type": "code",
        "outputId": "ce2cd82b-a920-4900-f34b-55b21348b31a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "texts = data['comment_text'].values\n",
        "target = data['should_ban'].values\n",
        "data[50::200]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>should_ban</th>\n",
              "      <th>comment_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0</td>\n",
              "      <td>\"Those who're in advantageous positions are th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>1</td>\n",
              "      <td>Fartsalot56 says f**k you motherclucker!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>450</th>\n",
              "      <td>1</td>\n",
              "      <td>Are you a fool? \\n\\nI am sorry, but you seem t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>650</th>\n",
              "      <td>1</td>\n",
              "      <td>I AM NOT A VANDAL!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>850</th>\n",
              "      <td>0</td>\n",
              "      <td>Citing sources\\n\\nCheck out the Wikipedia:Citi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     should_ban                                       comment_text\n",
              "50            0  \"Those who're in advantageous positions are th...\n",
              "250           1          Fartsalot56 says f**k you motherclucker!!\n",
              "450           1  Are you a fool? \\n\\nI am sorry, but you seem t...\n",
              "650           1    I AM NOT A VANDAL!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
              "850           0  Citing sources\\n\\nCheck out the Wikipedia:Citi..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4GeqLCPTyQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "texts_train, texts_test, y_train, y_test = train_test_split(texts, target, test_size=0.5, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ47nBhCTyQu",
        "colab_type": "text"
      },
      "source": [
        "__Note:__ it is generally a good idea to split data into train/test before anything is done to them.\n",
        "\n",
        "It guards you against possible data leakage in the preprocessing stage. For example, should you decide to select words present in obscene tweets as features, you should only count those words over the training set. Otherwise your algoritm can cheat evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPexLCWdTyQw",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing and tokenization\n",
        "\n",
        "Comments contain raw text with punctuation, upper/lowercase letters and even newline symbols.\n",
        "\n",
        "To simplify all further steps, we'll split text into space-separated tokens using one of nltk tokenizers.\n",
        "\n",
        "Generally, library `nltk` [link](https://www.nltk.org) is widely used in NLP. It is not necessary in here, but mentioned to intoduce it to you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1o-fqJfTyQx",
        "colab_type": "code",
        "outputId": "0e6cd140-813d-4ce4-831f-16491aa68d86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tokenizer = TweetTokenizer()\n",
        "preprocess = lambda text: ' '.join(tokenizer.tokenize(text.lower()))\n",
        "\n",
        "text = 'How to be a grown-up at work: replace \"I don\\'t want to do that\" with \"Ok, great!\".'\n",
        "print(\"before:\", text,)\n",
        "print(\"after:\", preprocess(text),)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before: How to be a grown-up at work: replace \"I don't want to do that\" with \"Ok, great!\".\n",
            "after: how to be a grown-up at work : replace \" i don't want to do that \" with \" ok , great ! \" .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaPNuiz7TyRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# task: preprocess each comment in train and test\n",
        "\n",
        "texts_train = np.array(list(map(preprocess, texts_train))) #<YOUR CODE>\n",
        "texts_test = np.array(list(map(preprocess, texts_test))) #<YOUR CODE> "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUWrc_ZZV1rX",
        "colab_type": "code",
        "outputId": "3f789332-8982-4844-bd30-7347522ad7ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 29
        }
      },
      "source": [
        "texts_train[5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'who cares anymore . they attack with impunity .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpkQ5xrCTyRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Small check that everything is done properly\n",
        "assert texts_train[5] ==  'who cares anymore . they attack with impunity .'\n",
        "assert texts_test[89] == 'hey todds ! quick q ? why are you so gay'\n",
        "assert len(texts_test) == len(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynSC8MSTTyRi",
        "colab_type": "text"
      },
      "source": [
        "### Step 1: bag of words\n",
        "\n",
        "One traditional approach to such problem is to use bag of words features:\n",
        "1. build a vocabulary of frequent words (use train data only)\n",
        "2. for each training sample, count the number of times a word occurs in it (for each word in vocabulary).\n",
        "3. consider this count a feature for some classifier\n",
        "\n",
        "__Note:__ in practice, you can compute such features using sklearn. __Please don't do that in the current assignment, though.__\n",
        "* `from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7EYjAIaTyRn",
        "colab_type": "code",
        "outputId": "3d8b166d-e44c-4bdf-b6e8-bd3bd70a18b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "# task: find up to k most frequent tokens in texts_train,\n",
        "# sort them by number of occurences (highest first)\n",
        "k = min(10000, len(set(' '.join(texts_train).split())))\n",
        "\n",
        "#<YOUR CODE>\n",
        "\n",
        "bow_vocabulary = list(set(' '.join(texts_train).split()))  #<YOUR CODE>\n",
        "\n",
        "print('example features:', sorted(bow_vocabulary)[::100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "example features: ['!', '12:20', '300', '_', 'adorned', 'alternative', 'archive', 'average', 'benkner', 'bout', 'came', 'chest', 'combined', 'consumers', 'cricket', 'decisions', 'dickheads', 'domestic', 'eductaion', 'essentially', 'faggot', 'firms', 'frustrated', 'goal', 'hanibal', 'hip-hop', 'identified', 'infoboxes', 'issue', 'kindergarten', 'lets', 'lot', \"mclaren's\", 'moderator', 'naturally', 'noticeable', 'opposing', 'pdf', 'plant', 'pretoria', 'punctuation', 'rebels', 'repetative', 'riadh', 'schulz', 'shes', 'slit', 'spoof', 'stupid', 't', 'theoretical', 'topic', 'uglyness', 'userspace', 'wanted', 'wikieditor', 'year', 'ீ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYy2cQ95ZEWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_index = {word:index for index,word in enumerate(bow_vocabulary)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONeLLkauZMyS",
        "colab_type": "code",
        "outputId": "2701ee6f-734e-473a-d46a-c9246f5f8ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "word_to_index"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cultural': 0,\n",
              " 'had': 1,\n",
              " 'vandalise': 2,\n",
              " 'cache': 3,\n",
              " 'engaging': 4,\n",
              " 'is': 5,\n",
              " 'rationale': 6,\n",
              " 'intuitive': 7,\n",
              " 'aficionados': 8,\n",
              " 'mistaken': 9,\n",
              " 'referring': 10,\n",
              " 'citations': 11,\n",
              " 'draft': 12,\n",
              " 'speak': 13,\n",
              " 'blocks': 14,\n",
              " 'experimenting': 15,\n",
              " 'gangrel': 16,\n",
              " 'pleads': 17,\n",
              " 'arbcom': 18,\n",
              " 'oblivious': 19,\n",
              " 'flavours': 20,\n",
              " 'luckily': 21,\n",
              " 'goodness': 22,\n",
              " 'sounds': 23,\n",
              " 'fan': 24,\n",
              " 'invited': 25,\n",
              " 'not': 26,\n",
              " 'unblock': 27,\n",
              " 'krauze': 28,\n",
              " 'produce': 29,\n",
              " '2014': 30,\n",
              " 'useful': 31,\n",
              " 'deny': 32,\n",
              " 'allegations': 33,\n",
              " 'tenuous': 34,\n",
              " 'bag': 35,\n",
              " 'bullying': 36,\n",
              " 'fits': 37,\n",
              " 'play': 38,\n",
              " 'began': 39,\n",
              " 'seperate': 40,\n",
              " 'water': 41,\n",
              " 'admins': 42,\n",
              " 'cyberdyne': 43,\n",
              " 'roster': 44,\n",
              " 'expand': 45,\n",
              " 'suicide': 46,\n",
              " '→': 47,\n",
              " 'gets': 48,\n",
              " 'raped': 49,\n",
              " 'quiet': 50,\n",
              " \"'\": 51,\n",
              " 'target': 52,\n",
              " 'judaism': 53,\n",
              " 'enthusiastic': 54,\n",
              " 'irresponsibility': 55,\n",
              " 'perverse': 56,\n",
              " 'conclude': 57,\n",
              " 'sh': 58,\n",
              " 'counted': 59,\n",
              " 'layout': 60,\n",
              " 'symbols': 61,\n",
              " 'corroborate': 62,\n",
              " 'ah': 63,\n",
              " 'grammer': 64,\n",
              " 'whitewash': 65,\n",
              " 'worn': 66,\n",
              " 'honour': 67,\n",
              " 'theyre': 68,\n",
              " 'bullshit': 69,\n",
              " 'sockpuppet': 70,\n",
              " 'formatting': 71,\n",
              " 'january': 72,\n",
              " 'propsnamentsrccvc': 73,\n",
              " 'football': 74,\n",
              " 'conform': 75,\n",
              " 'process': 76,\n",
              " '50,000': 77,\n",
              " '¡': 78,\n",
              " 'european': 79,\n",
              " '9': 80,\n",
              " 'subtract': 81,\n",
              " 'contributed': 82,\n",
              " 'inaccuracies': 83,\n",
              " 'km2': 84,\n",
              " 'maintain': 85,\n",
              " '=|': 86,\n",
              " 'says': 87,\n",
              " 'song': 88,\n",
              " 'interwiki': 89,\n",
              " 'drawing': 90,\n",
              " 'j': 91,\n",
              " 'commit': 92,\n",
              " 'walsh': 93,\n",
              " 'bare': 94,\n",
              " 'click': 95,\n",
              " 'jannetty': 96,\n",
              " 'bryant': 97,\n",
              " 'giving': 98,\n",
              " 'எந': 99,\n",
              " 'ண': 100,\n",
              " 'esp': 101,\n",
              " 'profanity': 102,\n",
              " 'agarwals': 103,\n",
              " 'died': 104,\n",
              " 'mckenna': 105,\n",
              " 'role': 106,\n",
              " 'daivari': 107,\n",
              " 'propaganda': 108,\n",
              " 'de': 109,\n",
              " 'jstor': 110,\n",
              " 'corrections': 111,\n",
              " 'shifting': 112,\n",
              " 'sexy': 113,\n",
              " 'shouting': 114,\n",
              " 'poshname': 115,\n",
              " 'next': 116,\n",
              " 'lake': 117,\n",
              " 'components': 118,\n",
              " 'idea': 119,\n",
              " 'around': 120,\n",
              " 'gvhy': 121,\n",
              " 'higher-ups': 122,\n",
              " 'skin': 123,\n",
              " 'tripping': 124,\n",
              " 'red': 125,\n",
              " 'turd': 126,\n",
              " 'impossible': 127,\n",
              " 'dwarf': 128,\n",
              " 'adopted': 129,\n",
              " 'mixture': 130,\n",
              " 'street': 131,\n",
              " 'muljana': 132,\n",
              " 'claim': 133,\n",
              " 'continued': 134,\n",
              " 'basic': 135,\n",
              " 'worked': 136,\n",
              " 'lamellae': 137,\n",
              " 'den': 138,\n",
              " 'accordance': 139,\n",
              " 'worried': 140,\n",
              " 'extra': 141,\n",
              " 'personal': 142,\n",
              " 'shed': 143,\n",
              " 'attract': 144,\n",
              " 'any': 145,\n",
              " 'unfortunately': 146,\n",
              " 'father': 147,\n",
              " 'cepos': 148,\n",
              " 'puting': 149,\n",
              " 'sucks': 150,\n",
              " 'unrepentant': 151,\n",
              " 'leash': 152,\n",
              " 'indicate': 153,\n",
              " 'burke': 154,\n",
              " 'elucidate': 155,\n",
              " 'instance': 156,\n",
              " 'steel': 157,\n",
              " 'unique': 158,\n",
              " 'miscellanious': 159,\n",
              " 'serve': 160,\n",
              " 'derivatives': 161,\n",
              " 'voters': 162,\n",
              " 'suckin': 163,\n",
              " 'legend': 164,\n",
              " 'cult': 165,\n",
              " '#bad': 166,\n",
              " 'ban': 167,\n",
              " 'formating': 168,\n",
              " 'finish': 169,\n",
              " \"dude's\": 170,\n",
              " 'input': 171,\n",
              " 'code': 172,\n",
              " 'regurgitation': 173,\n",
              " 'apparent': 174,\n",
              " '?': 175,\n",
              " 'gleaned': 176,\n",
              " 'reviewer': 177,\n",
              " 'dispute': 178,\n",
              " 'activities': 179,\n",
              " 'historic': 180,\n",
              " 'nhl': 181,\n",
              " 'try': 182,\n",
              " 'popular': 183,\n",
              " 'ignore': 184,\n",
              " 'palmer': 185,\n",
              " 'commercial': 186,\n",
              " 'lame': 187,\n",
              " 'historically': 188,\n",
              " 'mei': 189,\n",
              " 'hundreds': 190,\n",
              " 'steve': 191,\n",
              " 'recommend': 192,\n",
              " 'heels': 193,\n",
              " 'calm': 194,\n",
              " 'journalism': 195,\n",
              " '51.122': 196,\n",
              " ';)': 197,\n",
              " 'throat': 198,\n",
              " '300': 199,\n",
              " 'formation': 200,\n",
              " 'homeland': 201,\n",
              " 'reported': 202,\n",
              " 'ntv': 203,\n",
              " 'everything': 204,\n",
              " 'eccentric': 205,\n",
              " 'all': 206,\n",
              " 'dunning-kruger': 207,\n",
              " 'love': 208,\n",
              " 'yr': 209,\n",
              " 'improved': 210,\n",
              " 'massive': 211,\n",
              " 'share': 212,\n",
              " 'category': 213,\n",
              " 'flimsy': 214,\n",
              " 'க': 215,\n",
              " 'eu': 216,\n",
              " 'thank': 217,\n",
              " 'touches': 218,\n",
              " 'baggage': 219,\n",
              " 'euro': 220,\n",
              " 'administrate': 221,\n",
              " 'sincerly': 222,\n",
              " 'þ': 223,\n",
              " 'countries': 224,\n",
              " 'godliness': 225,\n",
              " 'disadvantages': 226,\n",
              " 're-created': 227,\n",
              " 'misery': 228,\n",
              " 'salix': 229,\n",
              " 'niederlandish': 230,\n",
              " 'rolling': 231,\n",
              " 'legally': 232,\n",
              " 'besides': 233,\n",
              " 'sneering': 234,\n",
              " 'matter': 235,\n",
              " 'henry': 236,\n",
              " 'assembled': 237,\n",
              " 'sonisona': 238,\n",
              " 'talents': 239,\n",
              " 'features': 240,\n",
              " 'intervention': 241,\n",
              " 'benidorm': 242,\n",
              " 'started': 243,\n",
              " 'certain': 244,\n",
              " 'returned': 245,\n",
              " 'confusion': 246,\n",
              " 'enjoy': 247,\n",
              " 'titled': 248,\n",
              " 'mabye': 249,\n",
              " ');': 250,\n",
              " 'pretend': 251,\n",
              " 'match': 252,\n",
              " 'leary': 253,\n",
              " 'eats': 254,\n",
              " 'wdefcon': 255,\n",
              " 'hank': 256,\n",
              " 'manual': 257,\n",
              " 'never': 258,\n",
              " 'private': 259,\n",
              " 'raise': 260,\n",
              " 'truck': 261,\n",
              " 'answer': 262,\n",
              " 'bodhidarama': 263,\n",
              " 'neogotiate': 264,\n",
              " 'interest': 265,\n",
              " '20th': 266,\n",
              " 'shameful': 267,\n",
              " 'inappropriate': 268,\n",
              " 'corbett': 269,\n",
              " 'continue': 270,\n",
              " 'buried': 271,\n",
              " 'researchers': 272,\n",
              " 'friends-with-benefits': 273,\n",
              " 'function': 274,\n",
              " 'make': 275,\n",
              " 'standardization': 276,\n",
              " 'agree': 277,\n",
              " 'ai': 278,\n",
              " '442': 279,\n",
              " 'unfamiliar': 280,\n",
              " 'checking': 281,\n",
              " 'kn': 282,\n",
              " 'typing': 283,\n",
              " 'june': 284,\n",
              " '33': 285,\n",
              " 'bug': 286,\n",
              " '&': 287,\n",
              " 'seconds': 288,\n",
              " 'see': 289,\n",
              " 'available': 290,\n",
              " 'outsiders': 291,\n",
              " 'baby': 292,\n",
              " 'thor': 293,\n",
              " 'good': 294,\n",
              " 'rest': 295,\n",
              " 'dipped': 296,\n",
              " 'coats': 297,\n",
              " 'steelworks': 298,\n",
              " 'position': 299,\n",
              " 'acknowledgement': 300,\n",
              " 'thanks': 301,\n",
              " 'disingenuous': 302,\n",
              " ':p': 303,\n",
              " 'names': 304,\n",
              " 'differently': 305,\n",
              " 'identical': 306,\n",
              " 'alone': 307,\n",
              " 'deprived': 308,\n",
              " 'valencian': 309,\n",
              " 'ன': 310,\n",
              " 'kinds': 311,\n",
              " 'suspected': 312,\n",
              " 'render': 313,\n",
              " 'date': 314,\n",
              " 'didier': 315,\n",
              " 'jackoff': 316,\n",
              " 'el': 317,\n",
              " 'offer': 318,\n",
              " 'supporting': 319,\n",
              " 'vivant': 320,\n",
              " 'american': 321,\n",
              " 'trolls': 322,\n",
              " 'trig': 323,\n",
              " 'him': 324,\n",
              " 'images': 325,\n",
              " 'comfort': 326,\n",
              " 'fairly': 327,\n",
              " 'coachman': 328,\n",
              " 'background': 329,\n",
              " 'forgot': 330,\n",
              " 'new-found': 331,\n",
              " 'diakou': 332,\n",
              " 'tomstar': 333,\n",
              " 'silliness': 334,\n",
              " 'm.g.shapero': 335,\n",
              " 'wishes': 336,\n",
              " 'appeal': 337,\n",
              " 'harass': 338,\n",
              " 'previously': 339,\n",
              " 'cossacks': 340,\n",
              " 'kindly': 341,\n",
              " 'read': 342,\n",
              " 'fyromian': 343,\n",
              " 'stinks': 344,\n",
              " 'successfully': 345,\n",
              " 'yourself': 346,\n",
              " 'known': 347,\n",
              " 'ksa': 348,\n",
              " 'therefore': 349,\n",
              " 'included': 350,\n",
              " 'advice': 351,\n",
              " 'bio-warfare': 352,\n",
              " 'lew': 353,\n",
              " 'despotic': 354,\n",
              " 'annoyed': 355,\n",
              " 'send': 356,\n",
              " 'christopher': 357,\n",
              " 'tomfoolery': 358,\n",
              " 'seemed': 359,\n",
              " 'boning': 360,\n",
              " 'guys': 361,\n",
              " 'prize': 362,\n",
              " 'playground': 363,\n",
              " 'bellamy': 364,\n",
              " 'ticket': 365,\n",
              " 'file': 366,\n",
              " 'concerned': 367,\n",
              " 'smart': 368,\n",
              " 'edits.also': 369,\n",
              " 'neutral': 370,\n",
              " 'manhood': 371,\n",
              " 'material': 372,\n",
              " 'anything': 373,\n",
              " 'ways': 374,\n",
              " 'dictatorships': 375,\n",
              " 'figured': 376,\n",
              " 'nose': 377,\n",
              " 'gun': 378,\n",
              " 'glow': 379,\n",
              " 'crack': 380,\n",
              " 'delay': 381,\n",
              " 'disambiguation': 382,\n",
              " 'arguments': 383,\n",
              " 'guidlines': 384,\n",
              " 'willing': 385,\n",
              " 'reliable': 386,\n",
              " 'edits': 387,\n",
              " 'innocent': 388,\n",
              " 'che': 389,\n",
              " 'useless': 390,\n",
              " 'help': 391,\n",
              " 'out': 392,\n",
              " 'dance': 393,\n",
              " 'title': 394,\n",
              " 'laws': 395,\n",
              " '235.249': 396,\n",
              " \"ragib's\": 397,\n",
              " '06:05': 398,\n",
              " 'currant': 399,\n",
              " 'fix': 400,\n",
              " 'demonstrating': 401,\n",
              " 'abuses': 402,\n",
              " 'luckiest': 403,\n",
              " 'romanian': 404,\n",
              " 'wedge': 405,\n",
              " 'study': 406,\n",
              " 'build': 407,\n",
              " 'liar': 408,\n",
              " '05:20': 409,\n",
              " 'likely': 410,\n",
              " 'tiderolls': 411,\n",
              " 'sterile': 412,\n",
              " 'matereal': 413,\n",
              " 'ny': 414,\n",
              " 'soa-cah-toa': 415,\n",
              " 'bones': 416,\n",
              " \"world's\": 417,\n",
              " 'book': 418,\n",
              " 'admin': 419,\n",
              " '>': 420,\n",
              " 'irish': 421,\n",
              " 'honest': 422,\n",
              " 'publications': 423,\n",
              " 'motha': 424,\n",
              " 'contines': 425,\n",
              " 'dangerously': 426,\n",
              " 'earth': 427,\n",
              " 'refs': 428,\n",
              " 'dare': 429,\n",
              " 'resubmit': 430,\n",
              " 'ipa': 431,\n",
              " 'ethnicity': 432,\n",
              " 'variation': 433,\n",
              " 'butthurt': 434,\n",
              " 'lay': 435,\n",
              " 'comb-like': 436,\n",
              " 'post': 437,\n",
              " '}:': 438,\n",
              " 'dislike': 439,\n",
              " 'markit': 440,\n",
              " 'experience': 441,\n",
              " 'tao': 442,\n",
              " 'alexandor': 443,\n",
              " 'reaches': 444,\n",
              " 'shahid': 445,\n",
              " 'poop': 446,\n",
              " 'springboks': 447,\n",
              " 'collapse': 448,\n",
              " 'course': 449,\n",
              " 'functions': 450,\n",
              " 'connected': 451,\n",
              " 'difficult': 452,\n",
              " 'eastlake': 453,\n",
              " 'scent': 454,\n",
              " 'passages': 455,\n",
              " 'konw': 456,\n",
              " 'whomever': 457,\n",
              " 'battleis': 458,\n",
              " 'federation': 459,\n",
              " 'authenticity': 460,\n",
              " 'indeed': 461,\n",
              " '8p': 462,\n",
              " 'easier': 463,\n",
              " 'condemn': 464,\n",
              " 'forcing': 465,\n",
              " 'becuase': 466,\n",
              " '202.124': 467,\n",
              " 'who': 468,\n",
              " 'liebe': 469,\n",
              " 'upload': 470,\n",
              " 'expression': 471,\n",
              " 'monitoring': 472,\n",
              " 'assume': 473,\n",
              " 'if': 474,\n",
              " 'eruption': 475,\n",
              " 'equally': 476,\n",
              " 'delete': 477,\n",
              " 'slammed': 478,\n",
              " 'bum': 479,\n",
              " 'airline': 480,\n",
              " 'reality': 481,\n",
              " 'non-free': 482,\n",
              " 'homosexual': 483,\n",
              " 'singular': 484,\n",
              " 'accolades': 485,\n",
              " 'maximum': 486,\n",
              " '_': 487,\n",
              " 'blank': 488,\n",
              " 'ridiculous': 489,\n",
              " 'yours': 490,\n",
              " 'biographies': 491,\n",
              " 'chavo': 492,\n",
              " 'herakles': 493,\n",
              " 'shoulders': 494,\n",
              " 'forbes': 495,\n",
              " 'organ': 496,\n",
              " 'lets': 497,\n",
              " 'geek': 498,\n",
              " 'acquire': 499,\n",
              " 'pectines': 500,\n",
              " 'zu': 501,\n",
              " 'enforcer': 502,\n",
              " 'busking': 503,\n",
              " 'suburbs': 504,\n",
              " 'kwa': 505,\n",
              " 'may': 506,\n",
              " 'five': 507,\n",
              " ';': 508,\n",
              " 'profile': 509,\n",
              " 'noetica': 510,\n",
              " 'deliberately': 511,\n",
              " 'observe': 512,\n",
              " 'categories': 513,\n",
              " 'gas': 514,\n",
              " 'im': 515,\n",
              " 'moreover': 516,\n",
              " 'non-notable': 517,\n",
              " 'me': 518,\n",
              " 'omfg': 519,\n",
              " '169.144': 520,\n",
              " 'conversations': 521,\n",
              " '06:24': 522,\n",
              " 'exactly': 523,\n",
              " 'peremptory': 524,\n",
              " 'trolling': 525,\n",
              " 'perpetuating': 526,\n",
              " 'hitler': 527,\n",
              " 'common': 528,\n",
              " 'wass': 529,\n",
              " 'curious': 530,\n",
              " 'resurgence': 531,\n",
              " 'l5': 532,\n",
              " 'november': 533,\n",
              " 'beating': 534,\n",
              " 'still': 535,\n",
              " 'eminem': 536,\n",
              " 'mortar': 537,\n",
              " 'lmao': 538,\n",
              " 'represented': 539,\n",
              " 'arena': 540,\n",
              " 'for': 541,\n",
              " 'goaded': 542,\n",
              " 'here': 543,\n",
              " 'tiny': 544,\n",
              " 'reach': 545,\n",
              " '17:33': 546,\n",
              " 'horace': 547,\n",
              " 'fomos': 548,\n",
              " 'games': 549,\n",
              " 'hd': 550,\n",
              " 'no': 551,\n",
              " 'hmmm': 552,\n",
              " 'footbal': 553,\n",
              " 'lot': 554,\n",
              " 'cont': 555,\n",
              " 'burn': 556,\n",
              " 'rerasons': 557,\n",
              " 'geogre': 558,\n",
              " 'tangent': 559,\n",
              " '235.223': 560,\n",
              " 'disputed': 561,\n",
              " 'gustave': 562,\n",
              " 'amazing': 563,\n",
              " 'onanizing': 564,\n",
              " 'prices': 565,\n",
              " 'resolving': 566,\n",
              " 'dislikes': 567,\n",
              " 'snubbed': 568,\n",
              " 'consumer': 569,\n",
              " 'shouldnt': 570,\n",
              " 'boilerplate': 571,\n",
              " 'car': 572,\n",
              " 'land': 573,\n",
              " 'earrings': 574,\n",
              " 'harrasment': 575,\n",
              " '99.226': 576,\n",
              " 'watch': 577,\n",
              " 'rumours': 578,\n",
              " 'i': 579,\n",
              " \"we're\": 580,\n",
              " 'market': 581,\n",
              " 're': 582,\n",
              " 'hahaha': 583,\n",
              " 'face': 584,\n",
              " 'ridicules': 585,\n",
              " 'porn': 586,\n",
              " 'fanatic': 587,\n",
              " 'capital': 588,\n",
              " 'recent': 589,\n",
              " 'sons': 590,\n",
              " 'will': 591,\n",
              " 'impunity': 592,\n",
              " 'ecliptic': 593,\n",
              " 'smaller': 594,\n",
              " 'j.delanoy': 595,\n",
              " 'gloss': 596,\n",
              " 'pr': 597,\n",
              " 'furthermore': 598,\n",
              " 'century': 599,\n",
              " 'justine': 600,\n",
              " 'aplication': 601,\n",
              " 'vandalizedhans-hermann': 602,\n",
              " 'accord': 603,\n",
              " 'finn': 604,\n",
              " 'apart': 605,\n",
              " 'norman': 606,\n",
              " 'captain': 607,\n",
              " '#titleu': 608,\n",
              " 'dan': 609,\n",
              " 'adult': 610,\n",
              " 'unforced': 611,\n",
              " '45': 612,\n",
              " 'significance': 613,\n",
              " 'performing': 614,\n",
              " 'ephemeral': 615,\n",
              " '40': 616,\n",
              " 'controversy': 617,\n",
              " 'gone': 618,\n",
              " 'debacle': 619,\n",
              " 'bugging': 620,\n",
              " 'librarian': 621,\n",
              " 'greatest': 622,\n",
              " 'fast': 623,\n",
              " '\\x95': 624,\n",
              " 'weavers': 625,\n",
              " 'tay': 626,\n",
              " 'discuss': 627,\n",
              " 'crushed': 628,\n",
              " 'b': 629,\n",
              " 'noted': 630,\n",
              " 'dismissed': 631,\n",
              " 'disconnected': 632,\n",
              " 'small': 633,\n",
              " 'jr': 634,\n",
              " 'unnecessarily': 635,\n",
              " 'differ': 636,\n",
              " 'slavs': 637,\n",
              " 'haven': 638,\n",
              " 'tm': 639,\n",
              " 'gene': 640,\n",
              " 'consciousness': 641,\n",
              " 'bodhidharma': 642,\n",
              " 'fatass': 643,\n",
              " 'emanates': 644,\n",
              " 'promotional': 645,\n",
              " 'border': 646,\n",
              " 'enticing': 647,\n",
              " 'businesses': 648,\n",
              " 'save': 649,\n",
              " 'wall': 650,\n",
              " \"ya'll\": 651,\n",
              " 'between': 652,\n",
              " 'stupidly': 653,\n",
              " 'thankyou': 654,\n",
              " 't': 655,\n",
              " 'motar': 656,\n",
              " 'urbanarcheolgy': 657,\n",
              " 'members': 658,\n",
              " 'addicted': 659,\n",
              " 'supporters': 660,\n",
              " 'flummery': 661,\n",
              " 'trouble': 662,\n",
              " 'revealed': 663,\n",
              " 'express': 664,\n",
              " 'engaged': 665,\n",
              " 'uncovers': 666,\n",
              " 'vs': 667,\n",
              " 'occupation': 668,\n",
              " 'houses': 669,\n",
              " 'listed': 670,\n",
              " 'meanie': 671,\n",
              " 'rape': 672,\n",
              " '__': 673,\n",
              " 'gustafsson': 674,\n",
              " \"person's\": 675,\n",
              " 'pretoria': 676,\n",
              " 'president': 677,\n",
              " 'moar': 678,\n",
              " 'softly': 679,\n",
              " 'agreed-upon': 680,\n",
              " 'inactive': 681,\n",
              " 'perezhilton': 682,\n",
              " 'l': 683,\n",
              " 'pillockbrain': 684,\n",
              " 'violating': 685,\n",
              " 'fun': 686,\n",
              " 'referred': 687,\n",
              " 'base': 688,\n",
              " 'hour': 689,\n",
              " 'relations': 690,\n",
              " 'revaling': 691,\n",
              " 'sectiond': 692,\n",
              " 'correspondents': 693,\n",
              " 'challenge': 694,\n",
              " 'person': 695,\n",
              " 'highway': 696,\n",
              " 'unusual': 697,\n",
              " 'promoting': 698,\n",
              " '\"': 699,\n",
              " 'reversed': 700,\n",
              " 'rs': 701,\n",
              " 'sucker': 702,\n",
              " 'americans': 703,\n",
              " 'honestly': 704,\n",
              " '119.10': 705,\n",
              " 'disapprove': 706,\n",
              " 'well': 707,\n",
              " 'subjects': 708,\n",
              " 'thnk': 709,\n",
              " 'encylopedic': 710,\n",
              " 'gg': 711,\n",
              " 'al-mansoury': 712,\n",
              " 'territroy': 713,\n",
              " 'premiership': 714,\n",
              " 'firms': 715,\n",
              " 'phenomenon': 716,\n",
              " 'attack': 717,\n",
              " 'rex': 718,\n",
              " 'nearly': 719,\n",
              " 'do': 720,\n",
              " 'seriously': 721,\n",
              " 'mushrooms': 722,\n",
              " 'ga': 723,\n",
              " 'combination': 724,\n",
              " 'perhaps': 725,\n",
              " 'saw': 726,\n",
              " 'eisenhower': 727,\n",
              " 'rockwell': 728,\n",
              " 'severely': 729,\n",
              " 'discretion': 730,\n",
              " 'uk': 731,\n",
              " 'absolutely': 732,\n",
              " 'disrupt': 733,\n",
              " 'jacobs': 734,\n",
              " \"hitler's\": 735,\n",
              " 'keeping': 736,\n",
              " 'extreme': 737,\n",
              " \"bernanke's\": 738,\n",
              " 'notability': 739,\n",
              " 'stuff': 740,\n",
              " 'media': 741,\n",
              " 'dr': 742,\n",
              " 'girder': 743,\n",
              " 'sjömarken': 744,\n",
              " 'th': 745,\n",
              " 'bunch': 746,\n",
              " 'gutierrez': 747,\n",
              " 'reader': 748,\n",
              " 'finishing': 749,\n",
              " 'xd': 750,\n",
              " 'secant': 751,\n",
              " '377': 752,\n",
              " 'motivated': 753,\n",
              " 'wrap': 754,\n",
              " 'refrence': 755,\n",
              " 'manga': 756,\n",
              " 'retrn': 757,\n",
              " 'phrase': 758,\n",
              " 'holdwater': 759,\n",
              " 'jonathan': 760,\n",
              " 'word': 761,\n",
              " 'outdated': 762,\n",
              " 'storyline': 763,\n",
              " 'weight': 764,\n",
              " 'resurrected': 765,\n",
              " 'randi': 766,\n",
              " 'am': 767,\n",
              " 'show': 768,\n",
              " 'friends': 769,\n",
              " 'comments': 770,\n",
              " \"won't\": 771,\n",
              " 'descriptors': 772,\n",
              " 'developed': 773,\n",
              " 'managers': 774,\n",
              " 'nickname': 775,\n",
              " 'fagplease': 776,\n",
              " 'conspiracy': 777,\n",
              " 'complaint': 778,\n",
              " 'monkeyzpop': 779,\n",
              " 'whose': 780,\n",
              " 'galloway': 781,\n",
              " 'pkwy': 782,\n",
              " 'twentieth': 783,\n",
              " 'hardly': 784,\n",
              " 'fundamentally': 785,\n",
              " 'religion': 786,\n",
              " 'tasty': 787,\n",
              " 'gerber': 788,\n",
              " 'feminist': 789,\n",
              " 'item': 790,\n",
              " 'can': 791,\n",
              " 'cunt': 792,\n",
              " '2010': 793,\n",
              " 'property': 794,\n",
              " 'fry': 795,\n",
              " 'cup': 796,\n",
              " 'cut': 797,\n",
              " 'fervent': 798,\n",
              " 'chart': 799,\n",
              " 'elijah': 800,\n",
              " 'klm': 801,\n",
              " 'star': 802,\n",
              " 'sagaiousphil': 803,\n",
              " '10:29': 804,\n",
              " '49.147': 805,\n",
              " 'npov': 806,\n",
              " 'report': 807,\n",
              " 'tournament': 808,\n",
              " \"what's\": 809,\n",
              " 'irrelevant': 810,\n",
              " 'results': 811,\n",
              " 'substantial': 812,\n",
              " 'sudanese': 813,\n",
              " 'மற': 814,\n",
              " 'how': 815,\n",
              " 'billy': 816,\n",
              " 'studies': 817,\n",
              " 'at': 818,\n",
              " 'man': 819,\n",
              " 'citation': 820,\n",
              " 'surat': 821,\n",
              " 'galton': 822,\n",
              " 'writings': 823,\n",
              " 'cd-r': 824,\n",
              " 'sphere': 825,\n",
              " 'carr': 826,\n",
              " 'grow': 827,\n",
              " 'leverne': 828,\n",
              " '13000': 829,\n",
              " 'political': 830,\n",
              " 'tags': 831,\n",
              " 'systematic': 832,\n",
              " 'from': 833,\n",
              " 'unwanted': 834,\n",
              " 'flowed': 835,\n",
              " 'stop': 836,\n",
              " 'elements': 837,\n",
              " 'butts': 838,\n",
              " 're-read': 839,\n",
              " 'blp': 840,\n",
              " 'malignant': 841,\n",
              " 'drug': 842,\n",
              " 'insists': 843,\n",
              " 'rock': 844,\n",
              " 'crime': 845,\n",
              " 'allows': 846,\n",
              " 'speedy': 847,\n",
              " 'organisation': 848,\n",
              " 'site': 849,\n",
              " '117.211': 850,\n",
              " 'vet': 851,\n",
              " 'shore': 852,\n",
              " 'eunuchs': 853,\n",
              " 'sign': 854,\n",
              " 'call': 855,\n",
              " 'cease': 856,\n",
              " 'figures': 857,\n",
              " 'barrymore': 858,\n",
              " '70': 859,\n",
              " 'terror': 860,\n",
              " 'childishly': 861,\n",
              " 'lack': 862,\n",
              " 'odd': 863,\n",
              " 'government': 864,\n",
              " 'illuminate': 865,\n",
              " 'concept': 866,\n",
              " 'affecting': 867,\n",
              " 'respect': 868,\n",
              " 'celtic': 869,\n",
              " 'ariva': 870,\n",
              " '#k_': 871,\n",
              " 'ஒர': 872,\n",
              " 'rambhadracharya': 873,\n",
              " 'ill': 874,\n",
              " 'cleavage': 875,\n",
              " 'ந': 876,\n",
              " 'canadian': 877,\n",
              " 'arbitrary': 878,\n",
              " 'theoretical': 879,\n",
              " 'corporate': 880,\n",
              " 'fuhrer': 881,\n",
              " 'conclusions': 882,\n",
              " 'citizens': 883,\n",
              " 'declined': 884,\n",
              " 'now': 885,\n",
              " 'suspicious': 886,\n",
              " 'curry': 887,\n",
              " 'interbreed': 888,\n",
              " 'listened': 889,\n",
              " 'connection': 890,\n",
              " 'thugs': 891,\n",
              " 'distribution': 892,\n",
              " 'bs': 893,\n",
              " 'tea': 894,\n",
              " 'baked': 895,\n",
              " 'relentless': 896,\n",
              " 'forever': 897,\n",
              " 'snottywong': 898,\n",
              " 'laughing': 899,\n",
              " 'solid': 900,\n",
              " 'percentage': 901,\n",
              " 'massively': 902,\n",
              " 'நன': 903,\n",
              " 'gogh': 904,\n",
              " 'frankly': 905,\n",
              " '29_the_computer_kilo': 906,\n",
              " ';-)': 907,\n",
              " 'dying': 908,\n",
              " 'vandalized': 909,\n",
              " 'un': 910,\n",
              " 'absolute': 911,\n",
              " 'huge': 912,\n",
              " 'ín': 913,\n",
              " 'crossed': 914,\n",
              " 'ideot': 915,\n",
              " '05:17': 916,\n",
              " 'intentions': 917,\n",
              " 'published': 918,\n",
              " 'talk': 919,\n",
              " 'election': 920,\n",
              " 'dravidian': 921,\n",
              " 'proposals': 922,\n",
              " 'mae': 923,\n",
              " 'tie': 924,\n",
              " 'silence': 925,\n",
              " 'counts': 926,\n",
              " '1420': 927,\n",
              " '::': 928,\n",
              " 'representation': 929,\n",
              " 'discretionary': 930,\n",
              " 'atrosity': 931,\n",
              " 'humor': 932,\n",
              " 'distance': 933,\n",
              " 'sparkling': 934,\n",
              " 'nah': 935,\n",
              " 'tamil': 936,\n",
              " 'culture': 937,\n",
              " 'da': 938,\n",
              " 'hugo': 939,\n",
              " 'mine': 940,\n",
              " 'truism': 941,\n",
              " 'dead': 942,\n",
              " 'chest': 943,\n",
              " 'mutha': 944,\n",
              " 'consistent': 945,\n",
              " \"that'll\": 946,\n",
              " 'again': 947,\n",
              " '10': 948,\n",
              " '15': 949,\n",
              " 'refusing': 950,\n",
              " 'something': 951,\n",
              " 'hate': 952,\n",
              " 'departed': 953,\n",
              " 'congress': 954,\n",
              " 'c': 955,\n",
              " 'super': 956,\n",
              " 'shout': 957,\n",
              " 'far': 958,\n",
              " 'complaining': 959,\n",
              " 'http://en.wikipedia.org/wiki/': 960,\n",
              " 'needed': 961,\n",
              " 'concern': 962,\n",
              " 'honoured': 963,\n",
              " 'bastards': 964,\n",
              " 'calls': 965,\n",
              " 'times': 966,\n",
              " 'natalya': 967,\n",
              " 'mud': 968,\n",
              " 'waiters': 969,\n",
              " 'pc': 970,\n",
              " 'documented': 971,\n",
              " 'deleted': 972,\n",
              " 'ass': 973,\n",
              " 'paraded': 974,\n",
              " 'lords': 975,\n",
              " 'reset': 976,\n",
              " 'struggle': 977,\n",
              " 'what': 978,\n",
              " 'source': 979,\n",
              " 'highly': 980,\n",
              " 'middle': 981,\n",
              " 'alllllllllllllllllllllllllllllll': 982,\n",
              " \"it'll\": 983,\n",
              " 'unfair': 984,\n",
              " 'city': 985,\n",
              " 'brief': 986,\n",
              " 'minority': 987,\n",
              " 'maj': 988,\n",
              " 'take': 989,\n",
              " 'total': 990,\n",
              " 'accidental': 991,\n",
              " 'crisis': 992,\n",
              " 'proviso': 993,\n",
              " 'result': 994,\n",
              " 'excess': 995,\n",
              " 'ring': 996,\n",
              " 'movie': 997,\n",
              " 'graduated': 998,\n",
              " 'sockpuppetry': 999,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okjZMi6DZh8W",
        "colab_type": "code",
        "outputId": "e0afbb3e-d4ce-4a19-8d56-8062624a9f56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 29
        }
      },
      "source": [
        "VOCABULARY_SIZE = len(bow_vocabulary)\n",
        "VOCABULARY_SIZE"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5722"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_9et_UoTyRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_to_bow(text):\n",
        "    \"\"\" convert text string to an array of token counts. Use bow_vocabulary. \"\"\"\n",
        "    #<YOUR CODE>\n",
        "    global VOCABULARY_SIZE, word_to_index\n",
        "    bow_vector = np.zeros(VOCABULARY_SIZE, dtype=float)\n",
        "\n",
        "    if isinstance(text, str) :\n",
        "        text = text.split(' ')\n",
        "\n",
        "    for word in text:\n",
        "        word_index = word_to_index.get(word, -1)\n",
        "        if word_index >= 0:\n",
        "            bow_vector[word_index] += 1.\n",
        "\n",
        "    return bow_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXovDuPcTyR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_bow = np.stack(list(map(text_to_bow, texts_train)))\n",
        "X_test_bow = np.stack(list(map(text_to_bow, texts_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh_KKjP_TySJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Small check that everything is done properly\n",
        "k_max = len(set(' '.join(texts_train).split()))\n",
        "assert X_train_bow.shape == (len(texts_train), min(k, k_max))\n",
        "assert X_test_bow.shape == (len(texts_test), min(k, k_max))\n",
        "assert np.all(X_train_bow[5:10].sum(-1) == np.array([len(s.split()) for s in  texts_train[5:10]]))\n",
        "assert len(bow_vocabulary) <= min(k, k_max)\n",
        "assert X_train_bow[6, bow_vocabulary.index('.')] == texts_train[6].split().count('.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs--TymETySY",
        "colab_type": "text"
      },
      "source": [
        "Now let's do the trick with `sklearn` logistic regression implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiIB4201TySa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "bow_model = LogisticRegression().fit(X_train_bow, y_train)   #<YOUR CODE>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kWGSgr0TySi",
        "colab_type": "code",
        "outputId": "4ce9793f-0412-4bfc-9eab-c5af70fe0718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "for name, X, y, model in [\n",
        "    ('train', X_train_bow, y_train, bow_model),\n",
        "    ('test ', X_test_bow, y_test, bow_model)\n",
        "]:\n",
        "    proba = model.predict_proba(X)[:, 1]\n",
        "    auc = roc_auc_score(y, proba)\n",
        "    plt.plot(*roc_curve(y, proba)[:2], label='%s AUC=%.4f' % (name, auc))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], '--', color='black',)\n",
        "plt.legend(fontsize='large')\n",
        "plt.grid()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deVzU1f7H8ddhcUEUDRQ3xCU3XFNyXzDLLc3Mq9dcyiKXrJtpZbaZmZW7ZteN1ELNyqxcfnkz64Z2yzW3XFIJQVEUUQFxheH8/hglUJABZuY7y+f5ePBg5rvN+zD48cuZ7/ccpbVGCCGE8/MwOoAQQgjrkIIuhBAuQgq6EEK4CCnoQgjhIqSgCyGEi/Ay6oUDAgJ09erVC7Xv5cuXKVWqlHUDOThps3uQNruHorT5999/T9Jal89tnWEFvXr16uzatatQ+0ZFRREWFmbdQA5O2uwepM3uoShtVkrF5bVOulyEEMJFSEEXQggXIQVdCCFchBR0IYRwEVLQhRDCReRb0JVSS5VSiUqpA3msV0qpuUqpaKXUfqVUM+vHFEIIkR9LztA/BbrdZX13oPbNr+HAgqLHEkIIUVD5Xoeutd6ilKp+l016A8u0eRzebUqpskqpSlrrBCtlzGHl9hNEbr/KgiNbbXF4h5WcLG12B9Jm1+Z59TztEj/nYsU2Nrn23ho3FlUBTmZ7Hn9z2R0FXSk1HPNZPIGBgURFRRX4xSK3X+VEqglILkxWp2UymUhOlja7Ommz/XjrdBTWnw9idPrHVM48Ayrn8m0xqUxad5w/i2fyr9EBhap/+bHrnaJa6wggAiA0NFQX5n8o8//kyWx8tbt1wzk4uZvOPUibbeByEsTvzLks+ifY+bHtXhOgRicAki+n88qXB1m8OY57A0sxO/x+Sja6nxYOeoZ+CgjK9rzqzWVCCJEv7xvJ8MObYEq3zQtsX5j3unZjoUQZ676e8oTG/aF0RUwmE20aNeLIkZOMGzeOiRMnUrJkSZucnYN1Cvo64Hml1BdASyDFVv3nQggHdS0F0q/efZs1oyAlHlTOvoi25/78+0kJP+tn8y4FgSHQY3rO5T4BUDYo932K6Pz589yjNZ6enrz33nsEBQURGhpqk9fKLt+CrpT6HAgDApRS8cDbgDeA1nohsAHoAUQDV4CnbBVWCOGAkk/C3KaQmWHZ9iG9czxN5B4q1GgI3aaCh3PfGqO15rPPPmP06NFMmTKFYcOG0adPH7u9viVXuTyez3oNPGe1REII53LlvLmYh4ZDxYZ5b6c8oO7D4Jtz5NdDUVFUcIHPDU6ePMnIkSPZsGEDrVq1om3btnbPYNjwuUIIF/DnBlg/2vy49kNQ170uVrjl888/Z8SIEZhMJubMmcPzzz+Pp6en3XNIQRdCFN7p3XA5EVqOhGqtjE5jmHLlytGyZUsiIiKoUaOGYTmkoAshCmfnYjj4rbkrpftUo9PYVUZGBrNnz+bGjRu88cYbdOvWja5du6Ju+8DX3qSgC+FsLhw3X1ttLxlXYUVf0Bo8i/29PP2y+Xuj/vbL4gD27dtHeHg4v//+O/3790drjVLK8GIOUtCFsI3UBNi3EjIzC7RbcOxx2Lwz7w2up8Jvc4sYrpDKVIUGj+Zc1uAxqNrcmDx2dv36dSZPnsyUKVO45557+Oqrr+jbt69DFPJbpKALYQt7P4P/vlvg3WoAxFqwYatRUKtzgY9faJ7eENzG/N1NHTt2jKlTpzJw4EBmzZqFv7+/0ZHuIAVdCFvQN8/M30w03zlooc2bN9OxY8e7b6QUeNj/Cgp3lJaWxtq1axk0aBANGzbkzz//pGbNmkbHypNzX8UvhKPz8AJPy7+0h2f+20kxt4tNmzbRqFEjhgwZwuHDhwEcupiDnKELYblrqZbdDbllBvzxle3zCJu4ePEiL7/8MkuXLqVOnTps3ryZ+vXrGx3LIlLQhcguYR+cj75zedxv5sv0CqLNC3I27WRMJhNt27bl6NGjvPbaa0yYMIESJUoYHctiUtCFSNgHO5cAGnYvu/u27V8C38D8j1mzE5SvY5V4wvaSkpK455578PT05P3336datWo0a+Z8s2lKQRfube3zsGe5+XHpSuDjD6FPQ6N+d25bwg9KV7RvPmFTWmuWL1/Oiy++yJQpUxg+fDiPPvpo/js6KCnowrlcjIO/frLe8Q6vg3I1oNWz0HKE9Y4rHF5cXBwjRoxg48aNtGnThg4dOhgdqcikoAvHFvureWYZfXOqsENrrP8aLYZLMXczK1as4Nlnn0VrzUcffcSoUaPwcPKhe0EKunAUF46bJ0nI5v4dz0FUvPlJQN2b3+tAlebw4EQrvbAC3wpWOpZwFuXLl6dt27YsWrSI4OBgo+NYjRR0YazUBPhlRq5XkJS69eCxj81TeglRSOnp6cycOZP09HTeeustunbtSpcuXRzqtn1rkIIujLV72d/FvOOrUKlp1qo/DhygUc8RtpmWTLiNPXv2EB4ezp49exgwYIBDDaZlbVLQhf2knYPEQzmXXfjL/P3NRPAqnmPV+TM+UsxFoV27do1JkyYxbdo0AgIC+Prrr3nssceMjmVTUtCF/XzzDMRE3bm8eJk7irkQRRUdHc2MGTN44oknmDlzJuXKlTM6ks1JQRe2s3Mx7F359/PEP6HyfdDlvZzblals31zCZaWlpfHtt98yZMgQGjZsyJEjRwydQcjepKAL2zm8HpKiIeh+8/Pg1tB4AFS3/+S5wvVt3LiR4cOHc/LkSUJDQ6lfv75bFXOQgi6sKfZXOPjN38/PHYUK9WDw18ZlEi7v/PnzjB07lmXLllGvXj1++eUXpxlMy9qkoIuCu3zePC3ZLVrD18/AyW3m5z7ZBv4PamHfbMKt3BpMKzo6mjfeeIM333zTqQbTsjYp6MJyl87C/2bD9gV5b/PoAmg60H6ZhFs6d+4c/v7+eHp6MnXqVIKDg2natGn+O7o4KejCctmLedvR4H/v3+uUJ9TtDj73GJNNuAWtNZ9++iljx45lypQpjBgxgt69exsdy2FIQRd5yzRB7C/w1VBAQfoVc3fKczuhlOPNpyhcW2xsLMOHD2fTpk20b9+eTp06GR3J4UhBF7mL2wpfh0PqKfPzCiEQ1NI8jooUc2Fny5cv59lnn0Upxfz58xkxYoRLDKZlbVLQRU7p12D9C7D/y7+XDfoaanWS2XeEYQIDA+nQoQMLFy6kWrVqRsdxWFLQhdnlJNgyHbYv/HvZwzOh2ZPg6W1cLuGW0tPTmTZtGiaTiQkTJtClSxe6dOlidCyHJwVdmH05GE5sNT8Oew1ajYISZYzNJNzS7t27efrpp9m3bx8DBw7MGkxL5E86odxdxg34YpB5Xs2qLeD5XRA2Xoq5sLurV68yfvx4WrRowdmzZ/n222/57LPPpJgXgEUFXSnVTSl1RCkVrZQan8v6akqpn5VSe5RS+5VSPawfVVhd+jU4sBr+/D8oW808a09AbaNTCTcVExPDrFmzGDp0KIcOHXLquT2Nkm+Xi1LKE5gHPATEAzuVUuu01tnHQX0TWKW1XqCUCgE2ANVtkFcUVuz/4M/vci7b+9nfswR1ngD1HrZ/LuHWUlNT+f777wkLC6NBgwYcO3bMpWYQsjdL+tBbANFa6xgApdQXQG8ge0HXwK2/0f2A09YMKazgf7Phr/9CMd+/l5nSobgfDF0PFRsbl024pQ0bNjBy5EhOnTrFk08+Sf369aWYF5ElBb0KcDLb83ig5W3bTAR+UEr9C/PMYQ/mdiCl1HBgOJgvQ4qKiipgXEhOvorJZCrUvs4sLS2tSG1ufP48Xr73srv59DtXHrkIRzYXPpyNFLXNzsgd2pySksK8efPYtGkTwcHBTJ06lbNnz3L27Fmjo9mNrd5na13l8jjwqdZ6plKqNbBcKdVQa52ZfSOtdQQQARAaGqrDwsIK/EILjmwlOTmZwuzrzKKiogrW5pgo2L/q7+cZZ6BMFaf6uRW4zS7A1dtsMpkICQkhJiaGCRMm8Prrr7N161aXbnNubPU+W1LQTwFB2Z5Xvbksu3CgG4DWeqtSqgQQACRaI6QohJ1L4MgGKF3J/NzDC2p0MDaTcFtnz56lfPnyeHp6MmPGDIKDg2ncWLr5rM2Sq1x2ArWVUjWUUsWAAcC627Y5AXQGUErVB0oA56wZVBSCf20Yc+DvrwffNjqRcDNaa5YsWULdunWJiIgAoFevXlLMbSTfM3StdYZS6nlgI+AJLNVaH1RKTQJ2aa3XAS8BHyulxmD+gHSo1lrbMrjIZst0OL0357L4XVDS9edQFI4rJiaGYcOG8d///peOHTvy4IO5frQmrMiiPnSt9QbMlyJmXzYh2+NDgMwrZpRf55q7VG51r4B5VMS63Y3LJNxaZGQko0aNwtPTk4ULFzJs2DAZTMsO5NZ/V9FkAHT7wOgUQgBQuXJlHnjgARYsWEDVqlWNjuM2pKA7s7RzsO55uJFmdBLh5m7cuMGUKVPIzMxk4sSJPPTQQzz00ENGx3I78jeQs9q+CGbcC0e/B28fqC3/eIQxdu7cSfPmzXn77beJiYlBPj4zjhR0Z5V0DLxKQtf3YdxxqPWA0YmEm7ly5Qovv/wyrVq14uLFi6xbt45ly5bJYFoGkoLujL5/HQ5+A94lofVz4FXM6ETCDR0/fpyPPvqIYcOGcfDgQXr16mV0JLcnfejOZudi2DbP/PiBN43NItxOSkoK33zzDU899RQNGjQgOjqaoKCg/HcUdiEF3ZGZMuDnyXD1InVOJ0DMVDjxGwTUhcGrzUPeCmEn3333HSNGjCAhIYHWrVtTr149KeYORrpcHNmFv8yjJB74Fv/zOyDpKJSrDo9/LsVc2M25c+cYNGgQPXv2pFy5cmzdupV69eoZHUvkQs7QHdmtqwV6zWFr0j1uN4CRMJ7JZKJdu3YcP36cd955h/Hjx1OsmHxm46ikoDuqs4fgx4lGpxBu6syZM1SoUAFPT09mzpxJ9erVadiwodGxRD6ky8URXUuFXUvg2Eao2Mj8JYQdZGZmsmjRIurUqcOiRYsA6NmzpxRzJyFn6I7oy0FwfAt4Fofhm8HDkztHLBbCuqKjoxk2bBhRUVE88MADdO3a1ehIooCkoBspMxP+bzSk3FasT+2BSk2h14c3i7kQtvXJJ58watQoihUrxscff0x4eLjcIOSEpKAbJdMEMT/D7mXgFwS+gX+vK18Xmg+Fyk0NiyfcS7Vq1ejatSvz5s2jSpUqRscRhSQF3Qjxv8OakebLEAHCxsN9g43NJNzK9evX+eCDD8jMzGTSpEl07tyZzp07Gx1LFJEUdHvQGtKvmL+vfQ4Orfl73eBvZGo4YVfbt28nPDycgwcP8uSTT6K1lu4VFyEF3dbOHIBvR8LZP3Iuf3gW3B9uTCbhli5fvsxbb73FnDlzqFKlCv/3f//Hww8/bHQsYUVS0G3lWir8MgN+/fDvZQ9NMs8s1Kg/+JY3LptwS3FxccyfP5+RI0cyZcoUypQpY3QkYWVS0G3lxDZzMfcsBq2fh7ajoWRZo1MJN5OcnMzq1at55plnCAkJITo6WmYQcmFS0G3h0hmIiTI/fvp7qNLc0DjCPa1du5Znn32WxMRE2rVrR7169aSYuzi5U9QWNk+7OcStgpL3GJ1GuJnExEQGDBjAo48+Svny5dm2bZsMpuUm5AzdFkzXzdeVP/sblAowOo1wIyaTibZt23LixAkmT57MuHHj8Pb2NjqWsBMp6Lbi4SXFXNjN6dOnqVixIp6ennz44YdUr16dkJAQo2MJO5MuF2vbPB3+ijI6hXATmZmZLFiwgHr16rFw4UIAevToIcXcTUlBtxat4exB85UtGVehQR+jEwkXd/ToUTp16sSoUaNo2bIl3bt3NzqSMJgUdGs5sx8WtIEbl6DZk9D1PaMTCRe2ZMkSmjRpwv79+1m6dCk//PADNWrUMDqWMJj0oVtD9E+w6W3z44fehdCnjc0jXF716tXp3r078+bNo1KlSkbHEQ5CCnpRXYiBnYsh8RDU7QFNB0FxX6NTCRdz/fp13n33XQAmT54sg2mJXEmXS1Gt/Ccc2QBlKpsnby7lb3Qi4WJ+++03mjZtynvvvUdCQgL61lyzQtxGCnpR3bgCdbrBMz8ZnUS4mLS0NEaPHk27du24cuUK33//PUuWLJGREUWeLCroSqluSqkjSqlopdT4PLbpr5Q6pJQ6qJRaad2YDs4nAEoH5r+dEAVw4sQJFi1axHPPPceBAwdkSjiRr3z70JVSnsA84CEgHtiplFqntT6UbZvawGtAW631RaVUBVsFFsKVXbp0iYiICIYPH05ISAgxMTFUrlzZ6FjCSVjyoWgLIFprHQOglPoC6A0cyrbNMGCe1voigNY60dpBhXB13377Lc888wwpKSl07NiRunXrSjEXBWJJQa8CnMz2PB5oeds2dQCUUr8CnsBErfX3tx9IKTUcGA4QGBhIVFRUgQMnJ1/FZDIVal9baHX9GhfPJHDExnnS0tIcps324i5tvnDhAnPnzmXz5s3UrFmTDz74gISEBBISEoyOZhfu8j5nZ6s2W+uyRS+gNhAGVAW2KKUaaa2Ts2+ktY4AIgBCQ0N1WFhYgV9owZGtJCcnU5h9bWJ3CSpVrEQlG+eJiopynDbbiTu02WQyUa9ePU6ePMn777/P/fffz4MPPmh0LLtyh/f5drZqsyUfip4CgrI9r3pzWXbxwDqtdbrW+jhwFHOBd11p52B2I0iNB7noQBRQfHw8mZmZeHp6MnfuXPbu3ctrr72Gl5fcGiIKz5KCvhOorZSqoZQqBgwA1t22zRrMZ+copQIwd8HEWDGn40mNh5QT5puJ7h9mdBrhJDIzM/noo4+oV68eCxYsAKB79+4yXrmwinwLutY6A3ge2AgcBlZprQ8qpSYppR65udlG4LxS6hDwM/CK1vq8rUI7lGZPQOWmRqcQTuDPP/+kQ4cOvPDCC7Rr146ePXsaHUm4GIv+vtNabwA23LZsQrbHGhh780sIcZvFixfz/PPP4+PjQ2RkJEOGDJEbhITVSYedEHZQq1YtevXqxb///W8CA+UmNGEbUtCFsIFr164xadIkAN5//306depEp06dDE4lXJ2M5SKElf366680bdqUDz74gHPnzslgWsJupKALYSWXLl3iX//6F+3bt+f69ets3LiRjz/+WPrKhd1IQRfCSuLj41m8eDH/+te/+OOPP+jSpYvRkYSbkT50IYrg/PnzrFq1imeffZb69esTExMjMwgJw8gZuhCFoLVm9erVhISE8MILL3DkyBEAKebCUFLQC+PQOlje5+YT6R91NwkJCfTt25d+/foRFBTErl27qFu3rtGxhJAul0JJ2AdXL0Kbf0Fwa6PTCDsymUy0b9+eU6dOMW3aNMaMGSPjrwiHIb+JBbVtIRxYDcoTukw2Oo2wk5MnT1KlShU8PT2ZN28eNWrUoE6dOkbHEiIH6XIpqF1L4WoyNH3c6CTCDkwmE3Pnzs0xmFbXrl2lmAuHJGfohVEzDHrPMzqFsLHDhw8THh7O1q1b6d69O7169TI6khB3JWfoBWFKB51pdAphBxERETRt2pSjR4+yfPlyvvvuO6pVq2Z0LCHuSgq6JdKvwr4v4d0AOH8MPOQPG1dXu3Zt+vTpw6FDhxg8eLDc7SmcglQmS/weCd+/an5cvT10HGdsHmF1V69eZeLEiSilmDJligymJZySnKFbIv2K+fuILfDkeigv1xy7ki1bttCkSROmTZtGSkqKDKYlnJYU9PycO2q+7hwgoC7In94uIzU1lVGjRtGxY0dMJhM//fQTCxYskO4V4bSkyyUvmZnw0zvw6xzz8xJ+0nfuYk6fPs2nn37K2LFjmTRpEqVKlTI6khBFIhUqL2lnzMW8RFlo+Bh0fhs85cfl7JKSkli1ahWjRo2iXr16HD9+XGYQEi5Dulzy89A70HM2lCxrdBJRBFprvvzyS0JCQnjxxRc5evQogBRz4VKkoAuXd/r0aR599FEGDBhAcHAwv//+u9zpKVyS9CEIl2YymejQoQOnTp1ixowZjB49WgbTEi5LfrOFS4qLi6Nq1ap4enoyf/58atasyb333mt0LCFsSrpchEsxmUzMmjWL+vXrZw2m1aVLFynmwi3IGbpwGQcOHCA8PJwdO3bQs2dPHn30UaMjCWFXUtBvd/wXOPo93EgzOokogIULF/LCCy/g5+fHypUrGTBggNwgJNyOFPTb/TIDYjaDt4/5GvQAuRrCkWmtUUpRv359+vXrx5w5cyhfvrzRsYQwhBR0gIzrcGwTmK7DpbNQrRU8/b3RqcRdXLlyhQkTJuDp6cnUqVPp2LEjHTt2NDqWEIaSD0UBDq+HLwfB6qfh3GEoeY/RicRdREVF0bhxY2bOnElaWpoMpiXETXKGDuYzdIAha6BMZSgrExk4opSUFMaNG0dERAS1atXiv//9rwxxK0Q2coae3T01zUPjepc0OonIRUJCAitWrODll19m//79UsyFuI1FBV0p1U0pdUQpFa2UGn+X7foqpbRSKtR6EYU7O3fuHB999BEA9erVIzY2lunTp+Pj42NwMiEcT74FXSnlCcwDugMhwONKqZBctisNjAa2WzukTd24AsknjE4hbqO15scff6R+/fq89NJLWYNpyRUsQuTNkjP0FkC01jpGa30D+ALonct27wJTgWtWzGd7342FzVPMj6WrxSGcPHmSXr168d5773HvvfeyZ88eGUxLCAtY8qFoFeBktufxQMvsGyilmgFBWuvvlFKv5HUgpdRwYDiYhy2NiooqcODk5KuYTKZC7ZubhvF/UapEIIfrv0TqrkPAIasc19rS0tKs1mZHZjKZeOKJJ7hw4QLPPPMMAwYM4Ny5c27RdnCf9zk7abP1FPkqF6WUBzALGJrftlrrCCACIDQ0VIeFhRX49RYc2UpycjKF2TdXCYvA+yrNeo+wzvFsJCoqynptdkCxsbEEBQXh6elJZGQkNWvW5MSJEy7d5ty4+vucG2mz9VjS5XIKCMr2vOrNZbeUBhoCUUqpWKAVsE4+GBWWyMjIYMaMGdSvX5/58+cD8OCDD1KzZk2DkwnhfCw5Q98J1FZK1cBcyAcAA2+t1FqnAAG3niulooCXtda7rBtVuJr9+/cTHh7Orl276N27N3379jU6khBOLd8zdK11BvA8sBE4DKzSWh9USk1SSj1i64A2dXovXDpjdAq3NH/+fJo3b05cXBxffvkl3377LZUrVzY6lhBOzaI+dK31BmDDbcsm5LFtWNFj2UFmJizpYh6/pXp7o9O4jVuDaTVs2JABAwYwe/ZsAgIC8t9RCJEvN771X5uLeWi4eSJoYVOXL1/mzTffxMvLi+nTp9OhQwc6dOhgdCwhXIp73vqfcR0OfGN+XLoiFC9tbB4X99NPP9GoUSPmzJnD9evXZTAtIWzEPQv6X/+Fb54xPy4ldx7aSnJyMs888wwPPvggXl5ebNmyhblz58rEE0LYiHsW9FujKz6xFpoPNTSKKzt79ixffPEFr776Kvv27aN9e/msQghbcuM+dKBUBZCzRau6VcRHjx5N3bp1iY2NlQ89hbAT9ztDP7oR/jfL6BQuR2vNihUrCAkJYdy4cRw7dgxAirkQduR+Bf2PryDxMNTsJBNZWMmJEyd4+OGHGTJkCHXr1mXv3r3Url3b6FhCuB337HLxqwpPrDE6hUvIyMggLCyMxMRE5s6dy6hRo/D09DQ6lhBuyT0LuiiymJgYgoOD8fLy4uOPP6ZWrVpUr17d6FhCuDX36HLJzITjW+DI95B62ug0Ti0jI4OpU6cSEhLCvHnzAOjcubMUcyEcgHucoZ/YCpG9/n5e+T7jsjixvXv3Eh4ezu7du+nTpw/9+vUzOpIQIhvXLujrX4ST2+F6mvl573lQIQTKVTc0ljP697//zZgxY/D392f16tUyMqIQDsi1C/rhdVDMFyo3gRodoEEfKFbK6FRO5dZgWo0bN2bQoEHMmjWLe+65x+hYQohcuHZBB6j9EDw80+gUTictLY033ngDb29vZsyYIYNpCeEE3ONDUVEgP/zwAw0bNuSjjz4iPT1dBtMSwklIQRdZLl68yFNPPUXXrl0pUaIEW7Zs4cMPP5TBtIRwElLQRZbExERWr17Na6+9xt69e2nXrp3RkYQQBeC6fejnjoApw+gUDu/MmTN8/vnnjBkzJmswLX9/f6NjCSEKwTXP0M8ehHkt4HoKePsYncYhaa2JjIwkJCSE1157LWswLSnmQjgv1yzoV5PN3ztPgA6vGJvFAcXGxtKtWzeGDh1KSEiIDKYlhItwrS6X5BOw8Q3z9ecAVe+HEmWMzeRgMjIy6NSpE0lJScybN4+RI0fi4eGa/68L4W5cq6Cv/CckHjI/7vIeVG1hbB4HEh0dTY0aNfDy8mLp0qXUrFmT4OBgo2MJIazItU7NblyGWp1hzCFo8zx4lzA6keHS09N5//33adCgQdZgWp06dZJiLoQLcq0zdDBP+uxXxegUDmH37t2Eh4ezd+9e+vXrxz//+U+jIwkhbMi1ztBFlrlz59KiRQvOnDnDN998w6pVqwgMDDQ6lhDChqSgu5hbt+nfd999PPHEExw6dIg+ffoYnEoIYQ+u1+Xipi5dusRrr71G8eLFmTlzJu3bt6d9+/ZGxxJC2JGcobuA77//noYNGzJ//ny01jKYlhBuSgq6Ezt//jxPPvkk3bt3p1SpUvz666/MmjVLBtMSwk1JQXdi58+f59tvv+Wtt95iz549tG7d2uhIQggDWVTQlVLdlFJHlFLRSqnxuawfq5Q6pJTar5T6SSklFznbSEJCAjNmzEBrTZ06dYiLi2PSpEkUL17c6GhCCIPlW9CVUp7APKA7EAI8rpQKuW2zPUCo1roxsBqYZu2g7k5rzdKlS6lfvz5vvfUW0dHRAJQrV87gZEIIR2HJGXoLIFprHaO1vgF8AfTOvoHW+met9ZWbT7cBVa0b070dP36cV155hfDwcJo0acK+fftkMC0hxB0suWyxCnAy2/N4oOVdtg8H/pPbCqXUcGA4QGBgIFFRUZalzCY5+SomkynXfVteu0bK2TP8WYjjOiqTycTgwYNJSUlgVdMAABhRSURBVElhzJgx9OzZk9OnT3P69Gmjo9lcWlpaoX5HnJm02T3Yqs1WvQ5dKTUYCAU65rZeax0BRACEhobqsLCwAr/GgiNbSU5OJtd995agZGBFKhbiuI7m2LFj1KxZE09PTz7//HMSExPp37+/0bHsKioqKvf32YVJm92DrdpsSUE/BQRle1715rIclFIPAm8AHbXW160Tz0KmdNg8Da5csOvL2kJ6ejpTp07l3XffZdq0aYwePZqwsDC3O4NxBpmZmcTHx3P58mWrHdPPz4/Dhw9b7XjOQNqck7e3NxUqVKBMmYIP/W1JQd8J1FZK1cBcyAcAA7NvoJS6D1gEdNNaJxY4RVGd+xO2TINivlClud1f3lp27dpFeHg4+/fvZ8CAATz++ONGRxJ3kZSUhFKKunXrWm1M+UuXLlG6dGmrHMtZSJv/prXm6tWrnDplPmcuaFHP97dQa50BPA9sBA4Dq7TWB5VSk5RSj9zcbDrgC3yllNqrlFpXoBRFdevOyD6LoOVwu760tXz44Ye0bNmSpKQk1q5dy+eff06FChWMjiXuIjk5mcDAQJkgRFiNUgofHx+qVKlCYmLBz40t6kPXWm8ANty2bEK2xw8W+JUFYP4fWSlFaGgo4eHhTJs2jbJlyxodS1jAZDLh7e1tdAzhgkqWLEl6enqB95PBuQySmprKq6++SokSJZg9ezZt27albdu2RscSBSTDLAhbKOzvlfytaIANGzbQoEEDIiIi8PLyksG0hBBWIQXdjpKSkhg8eDAPP/wwfn5+/Pbbb0yfPl3O8oRDGjlyJO+++67RMUQBSEG3o4sXL7J+/Xrefvttdu/eTcuWd7s/S4jCq169Oj/++GORjrFw4ULeeuutIh1j6NCheHl5kZCQcMfyN998M8ey2NhYlFJkZGRkLVu5ciWhoaH4+vpSqVIlunfvzv/+978CZThw4ABdu3YlICDAopOnvXv30rx5c3x8fGjevDl79+7NWqe15tVXX8Xf3x9/f39effXVHH9hW7pvcHDwHftagxR0Gzt16hTTpk1Da03t2rWJi4tj4sSJFCtWzOhowo1lL5q2cvnyZb7++mv8/PxYsWJFgfefNWsWL774Iq+//jpnz57lxIkTjBo1irVr1xboON7e3vTv358lS5bku+2NGzfo3bs3gwcP5uLFizz55JP07t2bGzduABAREcGaNWvYt28f+/fvZ/369SxatKjA+27dujXHvlZza0IEe381b95cF0b/hb/pLlM2/L0g9jetF3XU+u0yWh9aX6hj2kJmZqaOiIjQZcqU0SVLltTHjh0r0vF+/vln6wRzIo7e5kOHDln9mKmpqUU+xuDBg7VSSpcoUUKXKlVKT506VR8/flwDevHixTooKEi3b99ea631P/7xDx0YGKjLlCmj27dvrw8cOJB1nCeffFK/8cYbWmvze1GlShU9Y8YMXb58eV2xYkW9dOnSu+aIjIzUVatW1XPmzNENGjTIsS77sW+1+VbG9PR0nZycrEuVKqVXrVpV5J/HLceOHdPmkpe3jRs36sqVK+vMzMysZUFBQfo///mP1lrr1q1b60WLFmWtW7x4sW7ZsmWB901NTc2xb27y+v0Cduk86qpzX+WSfBJ2LobTe6B2V6h8n9GJAPjrr78YNmwYP//8M2FhYXz88cfce++9RscSNvbO+oMcOp1apGOYTCY8PT3zXB9SuQxv92pw12MsX76cX375hcWLF/Pgg+YrimNjYwHYvHkzhw8fzrp2vnv37ixdupRixYrx6quvMmjQoBzdBNmdOXOGlJQUTp06xaZNm/jHP/7Bo48+mueIn5GRkTz++OMMGDCAl156id9//53mzS278W/r1q1cu3btrvPhrly5klGjRuW5fv/+/VSrVs2i17vl4MGDNG7cOEfXTOPGjTl48CDdunXj4MGDNGnSJGtdkyZNOHjwYJH3tRbnLuirn4L4nVDcDwatMjoNYP5TtnPnzly4cIFFixbxzDPPyI0nwmFMnDiRUqVKZT1/+umnc6wrV64cKSkp+Pn53bGvt7c3EyZMwMvLix49euDr68uRI0do1arVHdueOHGCn3/+mZkzZxIYGEjnzp1ZtmyZxQX9/PnzBAQE4OWVd4kaOHAgAwcOzHN9YaSlpd3Rdj8/Py5dupTrej8/P9LS0tBaF2lfa10Y4dwF/cYVCG4Hj0UYnYQjR45Qq1YtvLy8iIyMpFatWlStKqMIu5P8zpwtYevb4IOC/h6WyWQy8cYbb/DVV19x7ty5rBOPpKSkXAu6v79/jgLr4+NDWlparq+zfPly6tevT9OmTQEYNGgQL730EjNmzMDb2xsvL687bpxJT0/Hw8MDDw8P/P39SUpKIiMj465F3dp8fX1JTc35V1ZqamrWe3L7+tTUVHx9fVFKFWlfa3H+U8eSZcGvimEvf+PGDd555x0aNWrEvHnzAOjYsaMUc2GovIpE9uUrV65k7dq1/Pjjj6SkpGR1y2grXHmxbNkyYmJiqFixIhUrVmTs2LEkJSWxYYP5hvNq1aplvd4tx48fJygoCA8PD1q3bk3x4sVZs2ZNnq/x2Wef4evrm+fXiRMnCpy7QYMG7N+/P8fPYP/+/TRo0CBr/b59+7LW7du3L8e6wu5rLc5f0A20Y8cOmjdvzsSJE+nXrx+DBg0yOpIQgHm+gZiYmLtuc+nSJYoXL46/vz9Xrlzh9ddft8prb926lb/++osdO3awd+9e9u7dy4EDBxg4cCDLli0DoG/fvnz33Xf88MMPmEwmTp8+zeTJkxkwYABg7o6YNGkSzz33HGvWrOHKlSukp6fzn//8h3HjxgHms/60tLQ8v271n2utuXbtWtbVJteuXeP69dwHhA0LC8PT05O5c+dy/fp1/v3vfwPwwAMPAPDEE08wa9YsTp06xenTp5k5cyZDhw4t8L4JCQk59rWavD4ttfWXVa5ymdda688HFuo4RTV79mzt4eGhq1Spotevt/3VNY5+xYctOHqbHfUqF621XrNmjQ4KCtJ+fn56+vTpOa4gueXSpUv6kUce0b6+vrpatWo6MjJSA1lXZOV2lUt2wcHBetOmTXe89ogRI/Rjjz12x/Lt27frYsWK6fPnz2uttV63bp1u1qyZLlOmjK5WrZp++eWX9ZUrV3Lss2LFCt28eXPt4+OjAwMDdY8ePfSvv/5aoJ/FrbZn/woODs5a361bN/3ee+9lPd+9e7du1qyZLlGihL7vvvv07t27s9ZlZmbqV155RZcrV06XK1dOv/LKKzmuarF037Jly96x7+0Kc5WL0gbddh4aGqp37dpV4P3+ucg8wcXGV7vD/DZwTw0Y8JkNEuZO3/wA47fffmPZsmVMnTo11/5Ga5NJABzP4cOHqV+/vlWPKUPJugdL2pzX75dS6netdWhu+zj3h6J2lJKSwrhx4yhZsiRz5syhTZs2tGnTxuhYQgiRRfrQLbB+/XpCQkJYvHgxxYsXl8G0hBAOSQr6XZw7d46BAwfyyCOP4O/vz7Zt25g6daoMpiWEcEhS0O8iJSWFDRs28M4777Br1y7uv/9+oyMJIUSenLIPvZS+DNNqwpXzEGDdW+pPnjzJihUrGD9+PPfeey9xcXF2+dBTCCGKyinP0P30JXMxr9cT2r9klWNmZmaycOFCGjRowOTJk/nrr7/MryXFXAjhJJyyoGep/whUapL/dvk4duwYDzzwAM8++ywtWrTgjz/+kMG0hBBOxym7XKwpIyODhx56iOTkZJYsWcJTTz0lH3oKIZyS2xb0w4cPU7t2bby8vFi+fDm1atWicuXKRscSQohCc+4ul0K4fv06b7/9No0bN84aa6F9+/ZSzIVLscYUdACffvop7dq1K9S+YWFhlCtX7o5xU8LCwli8eHGOZVFRUTkGtNNaM3fuXBo2bEipUqWoWrUq/fr1448//ihQBp3PlHG3++ijj6hRowZlypQhNDQ0x3R3s2fPpmbNmpQpU4bKlSszZsyYHDM/Va9enZIlS2YNDtalS5esdQWdBq+w3Kqgb9u2jWbNmjFp0iQef/xxhgwZYnQkIVxSbGwsv/zyC0op1q1bV+D9R48ezYcffsjcuXO5cOECR48e5dFHH+W7774r0HHuNmXc7bZv38748eNZvXo1KSkphIeH06dPH0wmEwCPPPIIu3fvJjU1lQMHDrBv3z7mzp2b4xjr16/PGhzshx9+yFpekGnwisJtCvrMmTNp06YNly5dYsOGDSxbtgx/f3+jYwlhdUOGDOHEiRP06tULX19fpk2bBphPaNq0aUPZsmVp0qQJUVFRWft8+umn1KxZk9KlS1OjRg0+++wzDh8+zMiRI9m6dSu+vr6ULVvW4gzLli2jVatWDB06lMjIyALlP3bsGPPmzePzzz/ngQceoHjx4vj4+DBo0CDGjx9foGNFRkby0ksvUbVqVapUqcJLL73Ep59+muu2sbGxNGjQgObNm6OU4oknniApKYnExEQAatWqlfUz0Frj4eFBdHS0RTnq1q1LeHi41YfLvZ3L96FnZmZmja88cuRIpkyZQpkyZYyOJVzRf8bDmYJ1CdyupCkDPO/yz7JiI+g+5a7HyG0KulOnTvHwww+zfPlyunXrxk8//UTfvn35888/8fHx4YUXXmDnzp3UrVuXhIQELly4QP369Vm4cCGLFy/O0fVgiWXLljF27FhatmxJq1atOHv2LIGBgRbt+9NPP1G1alVatGiR5zZTpkxhypS8fw7JyckABZr2rXv37kybNo3t27cTGhrK0qVLadq0KRUrVszaZuXKlYwcOZJLly4REBDAzJkzcxxj0KBBZGZmct999zF9+vQcr20PLnuGnpycTHh4OKNHjwagTZs2zJ8/X4q5cEsrVqygR48e9OjRAw8PDx566CFCQ0OzJpzw8PDgwIEDXL16lUqVKhXpTPJ///sfcXFx9O/fn+bNm1OrVi1Wrlxp8f7nz5+nUqVKd91m/PjxJCcn5/l1y92mfbtd6dKl6du3L+3ataN48eK88847RERE5OjzHjhwIKmpqRw9epSRI0fm+E/qs88+IzY2lri4ODp16kTXrl1zZLEHlzxDX7NmDaNGjSIxMZFx48ZZdc4+IfKUz5mzJa7aaCjZuLg4vvrqK9avX5+1LD09nU6dOlGqVCm+/PJLZsyYQXh4OG3btmXmzJnUq1evUK8VGRlJly5dCAgIAMxFMDIykjFjxgDkOf2ct7c3YJ7qLiEhoVCvfbuCTPu2ZMkSPvnkEw4ePMi9997LDz/8QM+ePdmzZ88dF03Url2bBg0aMGrUKL755hsA2rZtm7X+tddeIzIykl9++YVevXpZpS2WcKkz9MTERPr370+fPn0IDAxkx44dvP/++1LMhdu5/Xc+KCiIIUOG5DiLvXz5clafdNeuXdm0aRMJCQnUq1ePYcOG5Xqc/Fy9epVVq1axefPmrOnnZs+ezb59+7KmX8tr+rng4GAAOnfuTHx8PHebL+H999+/6/RztxRk2re9e/fSs2dP6tSpg4eHB926daNSpUr89ttvuW6fkZGRdUd5bpRSdh+Z1aUKempqKps2beK9995jx44dNGvWzOhIQhji9inoBg8ezPr169m4cSMmk4lr164RFRVFfHw8Z8+eZe3atVy+fJnixYvj6+ubNWF0YGAg8fHxWdO35WfNmjV4enpy6NChrOnnDh8+TPv27bOmn/vnP//JJ598wo4dO9Bac+zYMWbPnp01/Vzt2rUZNWoUjz/+OFFRUdy4cYNr167xxRdfZPWbv/7663edfu6Wu00Zd7v777+f7777jpiYGLTWbNq0iaNHj9KwYUMAFi9enPUB6aFDh/jggw/o3LkzACdOnODXX3/Nyjp9+nSSkpKyztp1AabBK5K8pjKy9VdRpqAb+v5Srd8uo/XeL3RcXJyePHly1lRO1prCy9E4+nRstuDobXamKei01nrbtm26Q4cOuly5cjogIED36NFDx8XF6dOnT+sOHTroMmXKaD8/P92xY0d98OBBrbXW169f1z169NDlypXT/v7++b5u165d9dixY+9Y/uWXX+rAwMCsKfCWLFmiQ0JCdOnSpXWNGjX0Bx98oE0mU9b2mZmZes6cOTokJESXLFlSV65cWffv318fOHCgQD+H/KaMK1WqlN6yZUvWtm+99ZYOCgrSvr6+ul69enrZsmVZ2w4dOlRXqFBB+/j46ODgYP3yyy/rq1evaq21PnDggG7UqJH28fHR99xzj37ggQf0zp07s/bNbxq83NhsCjqlVDfgQ8ATWKy1nnLb+uLAMqA5cB74p9Y69m7HLOwUdIMXRtHs/He8eGMRC9P78epHq8jMzGTfvn0uPf6Ko0/HZguO3maZgs46pM25K8wUdPl2uSilPIF5QHcgBHhcKRVy22bhwEWt9b3AbGBqfsctrE5XNvJwwgLCPr3Ccx8spXXr1lkfYgghhDuzpA+9BRCttY7RWt8AvgB637ZNb+DW3QOrgc7KRp9EemVcpeuKK/yR4sMnS5eyceNGqlevbouXEkIIp2LJZYtVgJPZnscDLfPaRmudoZRKAfyBpOwbKaWGA8PB/GFL9jvVLHXJO4A3+jUn4KGxlKtQic2bNxf4GM4oLS2tUD8vZ+bobfbz8+PSpUtWPabJZLL6MR2dtDl3tz64Lgi7XoeutY4AIsDch16Y/tGwsDCioto5dN+qLTh6f7ItOHqbDx8+nOc1zYUl/cnuIb82a60pUaIE9913X4GOa0mXyykgKNvzqjeX5bqNUsoL8MP84agQLsvT0/OOG2SEsIarV69m3WhVEJYU9J1AbaVUDaVUMWAAcPvwaeuAJ28+/gfwX23J5TNCOLGyZcty9uxZMjMzjY4iXITWmitXrnDq1CkqVKhQ4P3z7XK52Sf+PLAR82WLS7XWB5VSkzBfD7kOWAIsV0pFAxcwF30hXFpAQADx8fEcOXLEase8du0aJUqUsNrxnIG0OSdvb28CAwMLNe6URX3oWusNwIbblk3I9vga0K/Ary6EE/Pw8KBatWpWPWZUVFSB+02dnbTZelzq1n8hhHBnUtCFEMJFSEEXQggXIQVdCCFchEWDc9nkhZU6B8QVcvcAbrsL1Q1Im92DtNk9FKXNwVrr8rmtMKygF4VSaldeo425Kmmze5A2uwdbtVm6XIQQwkVIQRdCCBfhrAU9wugABpA2uwdps3uwSZudsg9dCCHEnZz1DF0IIcRtpKALIYSLcOiCrpTqppQ6opSKVkqNz2V9caXUlzfXb1dKVbd/SuuyoM1jlVKHlFL7lVI/KaWCjchpTfm1Odt2fZVSWinl9Je4WdJmpVT/m+/1QaXUSntntDYLfrerKaV+Vkrtufn73cOInNailFqqlEpUSh3IY71SSs29+fPYr5RqVuQX1Vo75BfmoXr/AmoCxYB9QMht24wCFt58PAD40ujcdmhzJ8Dn5uNn3aHNN7crDWwBtgGhRue2w/tcG9gDlLv5vILRue3Q5gjg2ZuPQ4BYo3MXsc0dgGbAgTzW9wD+AyigFbC9qK/pyGfoDjU5tZ3k22at9c9a6ys3n27DPIOUM7PkfQZ4F5gKXLNnOBuxpM3DgHla64sAWutEO2e0NkvarIFbg4D7AaftmM/qtNZbMM8PkZfewDJttg0oq5SqVJTXdOSCntvk1FXy2kZrnQHcmpzaWVnS5uzCMf8P78zybfPNP0WDtNbf2TOYDVnyPtcB6iilflVKbVNKdbNbOtuwpM0TgcFKqXjM8y/8yz7RDFPQf+/5susk0cJ6lFKDgVCgo9FZbEkp5QHMAoYaHMXevDB3u4Rh/itsi1KqkdY62dBUtvU48KnWeqZSqjXmWdAaaq1ljj8LOfIZujtOTm1Jm1FKPQi8ATyitb5up2y2kl+bSwMNgSilVCzmvsZ1Tv7BqCXvczywTmudrrU+DhzFXOCdlSVtDgdWAWittwIlMA9i5aos+vdeEI5c0N1xcup826yUug9YhLmYO3u/KuTTZq11itY6QGtdXWtdHfPnBo9orXcZE9cqLPndXoP57BylVADmLpgYe4a0MkvafALoDKCUqo+5oJ+za0r7Wgc8cfNql1ZAitY6oUhHNPqT4Hw+Je6B+czkL+CNm8smYf4HDeY3/CsgGtgB1DQ6sx3a/CNwFth782ud0Zlt3ebbto3Cya9ysfB9Vpi7mg4BfwADjM5shzaHAL9ivgJmL9DF6MxFbO/nQAKQjvkvrnBgJDAy23s87+bP4w9r/F7Lrf9CCOEiHLnLRQghRAFIQRdCCBchBV0IIVyEFHQhhHARUtCFEMJFSEEXQggXIQVdCCFcxP8DBBIMsy7RfRkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCmtxXD9TySu",
        "colab_type": "text"
      },
      "source": [
        "Seems alright. Now let's create the simple logistic regression using PyTorch. Just like in the classwork."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MnHPnM6TySv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIt8lwQ1TyS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from utils import plot_train_process"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5SzTZq5TyTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential()\n",
        "\n",
        "model.add_module('l1',  nn.Linear(VOCABULARY_SIZE, 2))  ### YOUR CODE HERE\n",
        "### YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1gTWyuhTyTM",
        "colab_type": "text"
      },
      "source": [
        "Remember what we discussed about loss functions! `nn.CrossEntropyLoss` combines both log-softmax and `NLLLoss`.\n",
        "\n",
        "__Be careful with it! Criterion `nn.CrossEntropyLoss` with still work with log-softmax output, but it won't allow you to converge to the optimum.__ Next comes small demonstration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaI0jHmTTyTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss_function = nn.NLLLoss()\n",
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXzrj99STyTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = torch.optim.Adagrad(model.parameters(), lr=0.001)\n",
        "### YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpBuAdvoTyTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_bow_torch = torch.tensor(X_train_bow, dtype=torch.float32 )     ### YOUR CODE HERE\n",
        "X_test_bow_torch = torch.tensor(X_test_bow, dtype=torch.float32 )       ### YOUR CODE HERE\n",
        "\n",
        "y_train_torch = torch.tensor(y_train, dtype=torch.float32 )             ### YOUR CODE HERE\n",
        "y_test_torch = torch.tensor(y_test, dtype=torch.float32 )               ### YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgYipX-tTyTm",
        "colab_type": "text"
      },
      "source": [
        "Let's test that everything is fine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83qjBgVpTyTo",
        "colab_type": "code",
        "outputId": "d88775c3-e4b5-421c-f792-d2cd5d62dc5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        }
      },
      "source": [
        "# example loss\n",
        "loss = loss_function(model(X_train_bow_torch[:3]), y_train_torch[:3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-404af8477e3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# example loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_bow_torch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_torch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 932\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfJLddoATyTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert type(loss.item()) == float"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlCSrCX4TyT-",
        "colab_type": "text"
      },
      "source": [
        "Here comes small function to train the model. In future we will take in into separate file, but for this homework it's ok to implement it here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnBytNbbTyUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    opt,\n",
        "    lr_scheduler,\n",
        "    X_train_torch,\n",
        "    y_train_torch,\n",
        "    X_val_torch,\n",
        "    y_val_torch,\n",
        "    n_iterations=500,\n",
        "    batch_size=32,\n",
        "    warm_start=False,\n",
        "    show_plots=True,\n",
        "    eval_every=10\n",
        "):\n",
        "    if not warm_start:\n",
        "        for name, module in model.named_children():\n",
        "            print('resetting ', name)\n",
        "            try:\n",
        "                module.reset_parameters()\n",
        "            except AttributeError as e:\n",
        "                print('Cannot reset {} module parameters: {}'.format(name, e))\n",
        "\n",
        "    train_loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_loss_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "    local_train_loss_history = []\n",
        "    local_train_acc_history = []\n",
        "    for i in range(n_iterations):\n",
        "\n",
        "        # sample 256 random observations\n",
        "        ix = np.random.randint(0, len(X_train_torch), batch_size)\n",
        "        x_batch = X_train_torch[ix]\n",
        "        y_batch = y_train_torch[ix]\n",
        "\n",
        "        # predict log-probabilities or logits\n",
        "        y_predicted = ### YOUR CODE\n",
        "\n",
        "        # compute loss, just like before\n",
        "        ### YOUR CODE\n",
        "\n",
        "\n",
        "        # compute gradients\n",
        "        ### YOUR CODE\n",
        "\n",
        "        # Adam step\n",
        "        ### YOUR CODE\n",
        "\n",
        "        # clear gradients\n",
        "        ### YOUR CODE\n",
        "\n",
        "\n",
        "        local_train_loss_history.append(loss.data.numpy())\n",
        "        local_train_acc_history.append(\n",
        "            accuracy_score(\n",
        "                y_batch.to('cpu').detach().numpy(),\n",
        "                y_predicted.to('cpu').detach().numpy().argmax(axis=1)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        if i % eval_every == 0:\n",
        "            train_loss_history.append(np.mean(local_train_loss_history))\n",
        "            train_acc_history.append(np.mean(local_train_acc_history))\n",
        "            local_train_loss_history, local_train_acc_history = [], []\n",
        "\n",
        "            predictions_val = model(X_val_torch)\n",
        "            val_loss_history.append(loss_function(predictions_val, y_val_torch).to('cpu').detach().item())\n",
        "\n",
        "            acc_score_val = accuracy_score(y_val_torch.cpu().numpy(), predictions_val.to('cpu').detach().numpy().argmax(axis=1))\n",
        "            val_acc_history.append(acc_score_val)\n",
        "            lr_scheduler.step(train_loss_history[-1])\n",
        "\n",
        "            if show_plots:\n",
        "                display.clear_output(wait=True)\n",
        "                plot_train_process(train_loss_history, val_loss_history, train_acc_history, val_acc_history)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HafIWRULTyUI",
        "colab_type": "text"
      },
      "source": [
        "Let's run it on the data. Note, that here we use the `test` part of the data for validation. It's not so good idea in general, but in this task our main goal is practice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEgN-QOaTyUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_model(model, opt, lr_scheduler, X_train_bow_torch, y_train_torch, X_test_bow_torch, y_test_torch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlwG7X5MTyUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "for name, X, y, model in [\n",
        "    ('train', X_train_bow_torch, y_train, model),\n",
        "    ('test ', X_test_bow_torch, y_test, model)\n",
        "]:\n",
        "    proba = model(X).detach().cpu().numpy()[:, 1]\n",
        "    auc = roc_auc_score(y, proba)\n",
        "    plt.plot(*roc_curve(y, proba)[:2], label='%s AUC=%.4f' % (name, auc))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], '--', color='black',)\n",
        "plt.legend(fontsize='large')\n",
        "plt.grid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GqteshFTyUf",
        "colab_type": "text"
      },
      "source": [
        "Try to vary the number of tokens `k` and check how the model performance changes. Show it on a plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S2MDLlxTyUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your beautiful code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At4UI0Q5TyUm",
        "colab_type": "text"
      },
      "source": [
        "### Step 2: implement TF-IDF features\n",
        "\n",
        "Not all words are equally useful. One can prioritize rare words and downscale words like \"and\"/\"or\" by using __tf-idf features__. This abbreviation stands for __text frequency/inverse document frequence__ and means exactly that:\n",
        "\n",
        "$$ feature_i = { Count(word_i \\in x) \\times { log {N \\over Count(word_i \\in D) + \\alpha} }}, $$\n",
        "\n",
        "\n",
        "where x is a single text, D is your dataset (a collection of texts), N is a total number of documents and $\\alpha$ is a smoothing hyperparameter (typically 1). \n",
        "And $Count(word_i \\in D)$ is the number of documents where $word_i$ appears.\n",
        "\n",
        "It may also be a good idea to normalize each data sample after computing tf-idf features.\n",
        "\n",
        "__Your task:__ implement tf-idf features, train a model and evaluate ROC curve. Compare it with basic BagOfWords model from above.\n",
        "\n",
        "__Please don't use sklearn/nltk builtin tf-idf vectorizers in your solution :)__ You can still use 'em for debugging though."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-59yTqwKTyUn",
        "colab_type": "text"
      },
      "source": [
        "Blog post about implementing the TF-IDF features from scratch: https://triton.ml/blog/tf-idf-from-scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3QVjICNTyUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your beautiful code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sphd6eRgTyUv",
        "colab_type": "text"
      },
      "source": [
        "Same stuff about moel and optimizers here (or just omit it, if you are using the same model as before)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KdNCCB8TyUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOJsI6ZkTyU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_tfidf_torch = ### YOUR CODE HERE\n",
        "X_test_tfidf_torch = ### YOUR CODE HERE\n",
        "\n",
        "y_train_torch = ### YOUR CODE HERE\n",
        "y_test_torch = ### YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ID_C81pTyVC",
        "colab_type": "text"
      },
      "source": [
        "Fit your model to the data. No not hesitate to vary number of iterations, learning rate and so on.\n",
        "\n",
        "_Note: due to very small dataset, increasing the complexity of the network might not be the best idea._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcJ2y3wdTyVD",
        "colab_type": "text"
      },
      "source": [
        "### Step 3: Comparing it with Naive Bayes\n",
        "\n",
        "Naive Bayes classifier is a good choice for such small problems. Try to tune it for both BOW and TF-iDF features. Compare the results with Logistic Regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7HWHxYxTyVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your beautiful code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfhz2Z6yTyVJ",
        "colab_type": "text"
      },
      "source": [
        "Shape some thoughts on the results you aquired. Which model has show the best performance? Did changing the learning rate/lr scheduler help?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtvCZ5AOTyVK",
        "colab_type": "text"
      },
      "source": [
        "_Your beautiful thoughts here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE9jknnVTyVM",
        "colab_type": "text"
      },
      "source": [
        "### Step 4: Using the external knowledge.\n",
        "\n",
        "Use the `gensim` word2vec pretrained model to translate words into vectors. Use several models with this new encoding technique. Compare the results, share your thoughts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxc2wUNkTyVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your beautiful code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmZ4VilzTyVY",
        "colab_type": "text"
      },
      "source": [
        "### Step 5: Visualizing the embeddings\n",
        "\n",
        "Finally, let's visualize the embeddings for every comment. One can use just averaged vector or use some more complex models. We recommend using `umap` (`pip install umap-learn`) to map everything to 2-dimensional space. For inspiration one might refer to the [day 14](https://github.com/neychev/harbour_ml2020/tree/master/day14_Unsupervised_learning) practice of the Machine Learning course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGfWotm9TyVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your beautiful code here"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}